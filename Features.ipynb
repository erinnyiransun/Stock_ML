{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Features.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPQkwDOiVUPNc8G7YZ7BgQR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atlantiquesun/Stock_ML/blob/main/Features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3dY7Vy8HZAm"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "import datetime\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS6NAWOfKm-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "118dc7fd-0044-4b59-d261-ac0cf9ed1838"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8CunY7lQo8L",
        "outputId": "4d61b5d3-07dc-4017-8899-84091327120e"
      },
      "source": [
        "os.listdir(\"/content/drive/MyDrive/StockML /Data/\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['papers.csv',\n",
              " 'companyInfo',\n",
              " 'starHistory',\n",
              " '.ipynb_checkpoints',\n",
              " 'forkHistory',\n",
              " 'issueHistory',\n",
              " 'commitHistory',\n",
              " 'issueClosedHistory',\n",
              " 'pullRequestClosedHistory',\n",
              " 'pullRequestMergedHistory',\n",
              " 'pullRequestHistory',\n",
              " 'processedData',\n",
              " 'trainData',\n",
              " 'financialData']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-oOFigNKogp"
      },
      "source": [
        "#calculate the daily cumulative for each company\n",
        "def calculate_cumulative(start=0, end=82):\n",
        "  '''\n",
        "  start: start company (index 0 to 81)\n",
        "  end: end company (the last company to be processed)\n",
        "  last end: 3 (elastic is not processed)\n",
        "  '''\n",
        "  companies = pd.read_csv(\"/content/drive/MyDrive/StockML /Data/companyInfo/companies_final.csv\")\n",
        "  dataCategories = []\n",
        "  for name in os.listdir(\"/content/drive/MyDrive/StockML /Data/\"):\n",
        "    if \"History\" in name: #if is a part of the raw data\n",
        "      dataCategories.append(name)\n",
        "\n",
        "  for i in range(companies.shape[0]):\n",
        "    if (i < start):continue\n",
        "    company = companies.at[i, 'githubUser']\n",
        "    normalizedName = companies.at[i, \"shortName\"]\n",
        "    print(company, normalizedName)\n",
        "    cumulativeData = {}\n",
        "    normalizedCmltData = {} #normalized cumulative data (each time series is normalized)\n",
        "    cumulativeData[\"date\"] = list(pd.date_range(start=\"1/01/1999\", end='9/01/2021').tz_localize(None)) #need to check the timezone\n",
        "    normalizedCmltData[\"date\"] = list(pd.date_range(start=\"1/01/1999\", end='9/01/2021').tz_localize(None))\n",
        "    #calculate the cumulative data for each category\n",
        "    for category in dataCategories:\n",
        "      print(category[:-7])\n",
        "      df = pd.read_csv(\"/content/drive/MyDrive/StockML /Data/\"+category+\"/\"+company+\".csv\")\n",
        "      df[\"sum\"] = df.sum(axis=1) #sum over the repositories\n",
        "      cumulativeData[category[:-7]] = list(df[\"sum\"])\n",
        "      #normalize data\n",
        "      sumDf = pd.DataFrame(df[\"sum\"])\n",
        "      nSumDf = (sumDf-sumDf.min())/(sumDf.max()-sumDf.min())\n",
        "      normalizedCmltData[category[:-7]] = list(nSumDf[\"sum\"])\n",
        "\n",
        "    cumulativeData = pd.DataFrame(cumulativeData)\n",
        "    cumulativeData.to_csv(\"/content/drive/MyDrive/StockML /Data/processedData/cumulativeData/\"+company+\".csv\")\n",
        "    normalizedCmltData = pd.DataFrame(normalizedCmltData)\n",
        "    normalizedCmltData.to_csv(\"/content/drive/MyDrive/StockML /Data/processedData/normalizedCumulativeData/\"+company+\".csv\")\n",
        "    if(i==end): break"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqQxlSlkjzrl"
      },
      "source": [
        "# Prepare Train Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u3UrfcmsNob"
      },
      "source": [
        "#1999/01/01 is a friday, so actually start from 1999/01/02 to 2021/04/30\n",
        "#currently a problem: the largest value in an entry (e.g. [1999-01-02, 'commit']) is not 1 because of the summation over a week\n",
        "\n",
        "def prepare_company_train_data(start=0, end=82, lag=1):\n",
        "  '''\n",
        "  cluster into weeks, concatenate github and stock data\n",
        "\n",
        "  lag: number of weeks the github data is lagged behind the stock data\n",
        "  '''\n",
        "  df_complete = None\n",
        "  companies = pd.read_csv(\"/content/drive/MyDrive/StockML /Data/companyInfo/companies_final.csv\")\n",
        "\n",
        "  for i in range(companies.shape[0]):\n",
        "    if (i < start): continue\n",
        "    company = companies.at[i, \"githubUser\"]\n",
        "    print(company)\n",
        "    ticker = companies.at[i, \"symbol\"].upper()\n",
        "    raw1 = pd.read_csv(\"/content/drive/MyDrive/StockML /Data/processedData/cumulativeData/\"+company+\".csv\")\n",
        "    raw2 = pd.read_csv(\"/content/drive/MyDrive/StockML /Data/processedData/normalizedCumulativeData/\"+company+\".csv\")\n",
        "\n",
        "    start_date = pd.to_datetime(\"1999-01-02\").tz_localize(None)\n",
        "    end_date = pd.to_datetime(\"2021-04-30\").tz_localize(None)\n",
        "    raw1['date'] = pd.to_datetime(raw1['date'])\n",
        "    raw2['date'] = pd.to_datetime(raw2['date'])\n",
        "    mask = (raw1['date'] >= start_date) & (raw1['date'] <= end_date) # ensure full weeks\n",
        "    raw1 = raw1.loc[mask].reset_index()\n",
        "    mask = (raw2['date'] >= start_date) & (raw2['date'] <= end_date)\n",
        "    raw2 = raw2.loc[mask].reset_index()\n",
        "\n",
        "    weekStarts = raw1[raw1.index%7==0].reset_index()\n",
        "    weekStarts = weekStarts['date'] #the dates on which a week starts (a Saturday)\n",
        "    weekEnds = raw1[raw1.index%7==6].reset_index()\n",
        "    weekEnds = weekEnds['date'] #the dates on which a week ends (a Friday)\n",
        "\n",
        "    raw1 = raw1.drop([\"index\", \"Unnamed: 0\"], axis = 1)\n",
        "    raw2 = raw2.drop([\"index\", \"Unnamed: 0\"], axis = 1)\n",
        "    raw2 = raw2.rename(columns = lambda s: 'n'+s)\n",
        "    raw = pd.concat([raw1, raw2], axis=1)\n",
        "\n",
        "    df = raw.groupby(np.floor(raw.index/7)).sum() #group into weeks\n",
        "    df['weekStarts'] = weekStarts\n",
        "    df['weekEnds'] = weekEnds\n",
        "\n",
        "    stock_data = pd.read_csv(\"/content/drive/MyDrive/StockML /Data/financialData/\"+ticker+\".csv\")\n",
        "    stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
        "\n",
        "    stock_data = stock_data[stock_data['Date'].isin(list(weekEnds))].reset_index() #select a week's close price\n",
        "    df = df[df['weekEnds'].isin(list(stock_data['Date']))].reset_index() #in some weeks there are no stock trade, or the stock was not yet on the market\n",
        "    \n",
        "    closePrices = list(stock_data['Close'])\n",
        "    weeklyReturn = [closePrices[0]]\n",
        "    for j in range(1, len(closePrices)):\n",
        "      wr = (closePrices[j]/closePrices[j-1])-1\n",
        "      weeklyReturn.append(wr)\n",
        "\n",
        "    df['weeklyReturn'] = pd.Series(weeklyReturn)\n",
        "    df['weeklyReturn'] = df['weeklyReturn'].shift(-lag)\n",
        "    print(df.shape[0])\n",
        "\n",
        "    df.to_csv(\"/content/drive/MyDrive/StockML /Data/trainData/companies/\"+company+\".csv\")\n",
        "    df['ticker'] = ticker\n",
        "    if(df_complete is None):\n",
        "      df_complete = df\n",
        "    else:\n",
        "      df_complete = pd.concat([df_complete, df])\n",
        "\n",
        "    if (i>=end):\n",
        "      break\n",
        "\n",
        "  df_complete.to_csv(\"/content/drive/MyDrive/StockML /Data/trainData/data.csv\") \n",
        "  return df_complete\n",
        "  "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3yDNaXdH0Al",
        "outputId": "8cc2efe1-3469-4487-afcb-7500470c5ee8"
      },
      "source": [
        "#calculate_cumulative(start=0, end=8)\n",
        "df = prepare_company_train_data(start=0, end=8, lag=1)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "amzn\n",
            "1125\n",
            "slackapi\n",
            "93\n",
            "cisco\n",
            "1125\n",
            "elastic\n",
            "129\n",
            "netflix\n",
            "955\n",
            "pinterest\n",
            "101\n",
            "shutterstock\n",
            "431\n",
            "intuit\n",
            "1125\n",
            "okta\n",
            "205\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "KRS8F1vLfum3",
        "outputId": "7de8b7af-6376-482d-d1d4-71ba2c5d978a"
      },
      "source": [
        "display(df)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>star</th>\n",
              "      <th>fork</th>\n",
              "      <th>issue</th>\n",
              "      <th>commit</th>\n",
              "      <th>issueClosed</th>\n",
              "      <th>pullRequestClosed</th>\n",
              "      <th>pullRequestMerged</th>\n",
              "      <th>pullRequest</th>\n",
              "      <th>nstar</th>\n",
              "      <th>nfork</th>\n",
              "      <th>nissue</th>\n",
              "      <th>ncommit</th>\n",
              "      <th>nissueClosed</th>\n",
              "      <th>npullRequestClosed</th>\n",
              "      <th>npullRequestMerged</th>\n",
              "      <th>npullRequest</th>\n",
              "      <th>weekStarts</th>\n",
              "      <th>weekEnds</th>\n",
              "      <th>weeklyReturn</th>\n",
              "      <th>ticker</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1999-01-02</td>\n",
              "      <td>1999-01-08</td>\n",
              "      <td>-0.124025</td>\n",
              "      <td>AMZN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1999-01-09</td>\n",
              "      <td>1999-01-15</td>\n",
              "      <td>-0.123776</td>\n",
              "      <td>AMZN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1999-01-16</td>\n",
              "      <td>1999-01-22</td>\n",
              "      <td>-0.049289</td>\n",
              "      <td>AMZN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1999-01-23</td>\n",
              "      <td>1999-01-29</td>\n",
              "      <td>-0.009086</td>\n",
              "      <td>AMZN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1999-01-30</td>\n",
              "      <td>1999-02-05</td>\n",
              "      <td>-0.098166</td>\n",
              "      <td>AMZN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>1159.0</td>\n",
              "      <td>32</td>\n",
              "      <td>12</td>\n",
              "      <td>19</td>\n",
              "      <td>275</td>\n",
              "      <td>18</td>\n",
              "      <td>123</td>\n",
              "      <td>84</td>\n",
              "      <td>117</td>\n",
              "      <td>1.684211</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.246753</td>\n",
              "      <td>0.838415</td>\n",
              "      <td>0.191489</td>\n",
              "      <td>1.782609</td>\n",
              "      <td>1.909091</td>\n",
              "      <td>2.294118</td>\n",
              "      <td>2021-03-20</td>\n",
              "      <td>2021-03-26</td>\n",
              "      <td>0.149729</td>\n",
              "      <td>OKTA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>1161.0</td>\n",
              "      <td>27</td>\n",
              "      <td>25</td>\n",
              "      <td>23</td>\n",
              "      <td>130</td>\n",
              "      <td>22</td>\n",
              "      <td>82</td>\n",
              "      <td>40</td>\n",
              "      <td>77</td>\n",
              "      <td>1.421053</td>\n",
              "      <td>0.520833</td>\n",
              "      <td>0.298701</td>\n",
              "      <td>0.396341</td>\n",
              "      <td>0.234043</td>\n",
              "      <td>1.188406</td>\n",
              "      <td>0.909091</td>\n",
              "      <td>1.509804</td>\n",
              "      <td>2021-04-03</td>\n",
              "      <td>2021-04-09</td>\n",
              "      <td>0.105298</td>\n",
              "      <td>OKTA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>1162.0</td>\n",
              "      <td>24</td>\n",
              "      <td>11</td>\n",
              "      <td>18</td>\n",
              "      <td>106</td>\n",
              "      <td>12</td>\n",
              "      <td>68</td>\n",
              "      <td>35</td>\n",
              "      <td>58</td>\n",
              "      <td>1.263158</td>\n",
              "      <td>0.229167</td>\n",
              "      <td>0.233766</td>\n",
              "      <td>0.323171</td>\n",
              "      <td>0.127660</td>\n",
              "      <td>0.985507</td>\n",
              "      <td>0.795455</td>\n",
              "      <td>1.137255</td>\n",
              "      <td>2021-04-10</td>\n",
              "      <td>2021-04-16</td>\n",
              "      <td>0.034521</td>\n",
              "      <td>OKTA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>1163.0</td>\n",
              "      <td>27</td>\n",
              "      <td>25</td>\n",
              "      <td>24</td>\n",
              "      <td>101</td>\n",
              "      <td>19</td>\n",
              "      <td>77</td>\n",
              "      <td>36</td>\n",
              "      <td>115</td>\n",
              "      <td>1.421053</td>\n",
              "      <td>0.520833</td>\n",
              "      <td>0.311688</td>\n",
              "      <td>0.307927</td>\n",
              "      <td>0.202128</td>\n",
              "      <td>1.115942</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>2.254902</td>\n",
              "      <td>2021-04-17</td>\n",
              "      <td>2021-04-23</td>\n",
              "      <td>-0.034372</td>\n",
              "      <td>OKTA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>1164.0</td>\n",
              "      <td>27</td>\n",
              "      <td>22</td>\n",
              "      <td>24</td>\n",
              "      <td>111</td>\n",
              "      <td>14</td>\n",
              "      <td>94</td>\n",
              "      <td>47</td>\n",
              "      <td>94</td>\n",
              "      <td>1.421053</td>\n",
              "      <td>0.458333</td>\n",
              "      <td>0.311688</td>\n",
              "      <td>0.338415</td>\n",
              "      <td>0.148936</td>\n",
              "      <td>1.362319</td>\n",
              "      <td>1.068182</td>\n",
              "      <td>1.843137</td>\n",
              "      <td>2021-04-24</td>\n",
              "      <td>2021-04-30</td>\n",
              "      <td>NaN</td>\n",
              "      <td>OKTA</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5289 rows Ã— 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index  star  fork  issue  ...  weekStarts   weekEnds  weeklyReturn  ticker\n",
              "0       0.0     0     0      0  ...  1999-01-02 1999-01-08     -0.124025    AMZN\n",
              "1       1.0     0     0      0  ...  1999-01-09 1999-01-15     -0.123776    AMZN\n",
              "2       2.0     0     0      0  ...  1999-01-16 1999-01-22     -0.049289    AMZN\n",
              "3       3.0     0     0      0  ...  1999-01-23 1999-01-29     -0.009086    AMZN\n",
              "4       4.0     0     0      0  ...  1999-01-30 1999-02-05     -0.098166    AMZN\n",
              "..      ...   ...   ...    ...  ...         ...        ...           ...     ...\n",
              "200  1159.0    32    12     19  ...  2021-03-20 2021-03-26      0.149729    OKTA\n",
              "201  1161.0    27    25     23  ...  2021-04-03 2021-04-09      0.105298    OKTA\n",
              "202  1162.0    24    11     18  ...  2021-04-10 2021-04-16      0.034521    OKTA\n",
              "203  1163.0    27    25     24  ...  2021-04-17 2021-04-23     -0.034372    OKTA\n",
              "204  1164.0    27    22     24  ...  2021-04-24 2021-04-30           NaN    OKTA\n",
              "\n",
              "[5289 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwF0LDuWrNWm"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhX8JcbIT3DY"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV,RandomizedSearchCV\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "\n",
        "def train_linear_regression(X_train,y_train):\n",
        "\n",
        "    lr_regressor = LinearRegression()\n",
        "    model = lr_regressor.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_lasso(X_train, y_train):\n",
        "    # lasso_regressor = Lasso()\n",
        "    # model = lasso_regressor.fit(X_train, y_train)\n",
        "\n",
        "    lasso = Lasso()\n",
        "    # scoring_method = 'r2'\n",
        "    # scoring_method = 'explained_variance'\n",
        "    scoring_method = 'neg_mean_absolute_error'\n",
        "    # scoring_method = 'neg_mean_squared_error'\n",
        "    #scoring_method = 'neg_mean_squared_log_error'\n",
        "    parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 1, 5, 10, 20]}\n",
        "    # my_cv_lasso = TimeSeriesSplit(n_splits=3).split(X_train_advanced)\n",
        "    lasso_regressor = GridSearchCV(lasso, parameters, scoring=scoring_method, cv=3)\n",
        "    lasso_regressor.fit(X_train, y_train)\n",
        "\n",
        "    model = lasso_regressor.best_estimator_\n",
        "    return model\n",
        "\n",
        "def train_ridge(X_train, y_train):\n",
        "    # lasso_regressor = Lasso()\n",
        "    # model = lasso_regressor.fit(X_train, y_train)\n",
        "\n",
        "    ridge = Ridge()\n",
        "    # scoring_method = 'r2'\n",
        "    # scoring_method = 'explained_variance'\n",
        "    scoring_method = 'neg_mean_absolute_error'\n",
        "    # scoring_method = 'neg_mean_squared_error'\n",
        "    #scoring_method = 'neg_mean_squared_log_error'\n",
        "    parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 1, 5, 10, 20]}\n",
        "    # my_cv_lasso = TimeSeriesSplit(n_splits=3).split(X_train_advanced)\n",
        "    ridge_regressor = GridSearchCV(ridge, parameters, scoring=scoring_method, cv=3)\n",
        "    ridge_regressor.fit(X_train, y_train)\n",
        "\n",
        "    model = ridge_regressor.best_estimator_\n",
        "    return model\n",
        "\n",
        "def train_random_forest(X_train, y_train):\n",
        "    '''\n",
        "    random_grid = {'bootstrap': [True, False],\n",
        "                   'max_depth': [10, 20, 40, 80, 100, None],\n",
        "                   'max_features': ['auto', 'sqrt'],\n",
        "                   'min_samples_leaf': [1, 2, 5, 10],\n",
        "                   'min_samples_split': [2, 5, 10],\n",
        "                   'n_estimators': [50, 200, 400, 600, 800, 1000, 1500]}\n",
        "    # my_cv_rf = TimeSeriesSplit(n_splits=5).split(X_train_rf)\n",
        "    rf = RandomForestRegressor(random_state=42)\n",
        "    randomforest_regressor = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,\n",
        "                                                cv=3, n_jobs=-1, scoring='neg_mean_absolute_error', verbose=0)\n",
        "    randomforest_regressor.fit(X_train, y_train)\n",
        "    model = randomforest_regressor.best_estimator_\n",
        "    '''\n",
        "    randomforest_regressor = RandomForestRegressor(n_estimators = 500, max_features=6)\n",
        "    #randomforest_regressor = RandomForestRegressor(random_state = 42,n_estimators = 300)\n",
        "\n",
        "    model = randomforest_regressor.fit(X_train, y_train)\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "def train_svm(X_train, y_train):\n",
        "    svr = SVR(kernel = 'rbf')\n",
        "\n",
        "    param_grid_svm = {'C':[0.001, 0.01, 0.1, 1, 10],'gamma': [1e-7, 1e-4,0.001,0.1]}\n",
        "    #param_grid_svm = {'kernel': ('linear', 'rbf','poly'), 'C':[0.001, 0.01, 0.1, 1, 10],'gamma': [1e-7, 1e-4,0.001,0.1],'epsilon':[0.1,0.2,0.5,0.3]}\n",
        "\n",
        "    # scoring_method = 'r2'\n",
        "    # scoring_method = 'explained_variance'\n",
        "    scoring_method = 'neg_mean_absolute_error'\n",
        "    # scoring_method = 'neg_mean_squared_error'\n",
        "    #scoring_method = 'neg_mean_squared_log_error'\n",
        "    \n",
        "    svm_regressor = GridSearchCV(estimator=svr, param_grid=param_grid_svm,\n",
        "                                       cv=3, n_jobs=-1, scoring=scoring_method, verbose=0)\n",
        "\n",
        "    svm_regressor.fit(X_train, y_train)\n",
        "    model = svm_regressor.best_estimator_\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_gbm(X_train, y_train):\n",
        "    '''gbm = GradientBoostingRegressor(random_state=42)\n",
        "    # model = gbm.fit(X_train, y_train)\n",
        "    param_grid_gbm = {'learning_rate': [0.1, 0.05, 0.01, 0.001], 'n_estimators': [100, 250, 500, 1000]}\n",
        "    # scoring_method = 'r2'\n",
        "    # scoring_method = 'explained_variance'\n",
        "    scoring_method = 'neg_mean_absolute_error'\n",
        "    # scoring_method = 'neg_mean_squared_error'\n",
        "    #scoring_method = 'neg_mean_squared_log_error'\n",
        "    gbm_regressor = RandomizedSearchCV(estimator=gbm, param_distributions=param_grid_gbm,\n",
        "                                       cv=3, n_jobs=-1, scoring=scoring_method, verbose=0)\n",
        "    gbm_regressor.fit(X_train, y_train)\n",
        "    model = gbm_regressor.best_estimator_'''\n",
        "    \n",
        "    gbm_regressor = GradientBoostingRegressor()\n",
        "    model = gbm_regressor.fit(X_train, y_train)\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_ada(X_train, y_train):\n",
        "    ada = AdaBoostRegressor(random_state=1)\n",
        "\n",
        "    # model = ada.fit(X_train, y_train)\n",
        "\n",
        "    param_grid_ada = {'n_estimators': [20, 50, 100],\n",
        "                      'learning_rate': [0.01, 0.05, 0.1, 0.3, 1],\n",
        "                      'loss' : ['linear', 'square', 'exponential']\n",
        "                     \n",
        "                     }\n",
        "    # scoring_method = 'r2'\n",
        "    # scoring_method = 'explained_variance'\n",
        "    scoring_method = 'neg_mean_absolute_error'\n",
        "    # scoring_method = 'neg_mean_squared_error'\n",
        "    #scoring_method = 'neg_mean_squared_log_error'\n",
        "\n",
        "    ada_regressor = GridSearchCV(estimator=ada, param_grid=param_grid_ada, cv=3, n_jobs=-1, scoring=scoring_method, verbose=0)\n",
        "\n",
        "    ada_regressor.fit(X_train, y_train)\n",
        "    model = ada_regressor.best_estimator_\n",
        "    '''\n",
        "    ada_regressor = AdaBoostRegressor()\n",
        "    model = ada_regressor.fit(X_train, y_train)\n",
        "    '''\n",
        "    return model\n",
        "\n",
        "def train_lstm(X_train, y_train, n_features=1):\n",
        "    \n",
        "    # Initialising the RNN\n",
        "    regressor = Sequential()\n",
        "    # Adding the first LSTM layer and some Dropout regularisation\n",
        "    regressor.add(LSTM(units = 80, return_sequences = True, input_shape = (X_train.shape[1], n_features)))\n",
        "    regressor.add(Dropout(0.2))\n",
        "\n",
        "    # Adding a second LSTM layer and some Dropout regularisation\n",
        "    regressor.add(LSTM(units = 40, return_sequences = True))\n",
        "    regressor.add(Dropout(0.2))\n",
        "\n",
        "    # Adding a third LSTM layer and some Dropout regularisation\n",
        "    regressor.add(LSTM(units = 20, return_sequences = False))\n",
        "    regressor.add(Dropout(0.2))\n",
        "\n",
        "    # Adding a fourth LSTM layer and some Dropout regularisation\n",
        "    #regressor.add(LSTM(units = 20,return_sequences = False))\n",
        "    #regressor.add(Dropout(0.2))\n",
        "\n",
        "    # Adding the output layer\n",
        "    regressor.add(Dense(units = 1, activation='linear'))\n",
        "    \n",
        "    #scoring_method = 'neg_mean_absolute_error'\n",
        "    # scoring_method = 'neg_mean_squared_error'\n",
        "    #scoring_method = 'neg_mean_squared_log_error'\n",
        "    # Compiling the RNN\n",
        "    regressor.compile(optimizer = 'adam', loss = 'mean_absolute_error')\n",
        "\n",
        "    # Fitting the RNN to the Training set\n",
        "    regressor.fit(X_train, y_train, epochs = 4, batch_size = 64)\n",
        "    print(regressor.summary())\n",
        "    return regressor"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xlki3Y2hgzm"
      },
      "source": [
        "# Training\n",
        "X_train = (samples * timesteps, features, 1) for lstm\n",
        "same as the original Big Data paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgPsKAA3hmQk"
      },
      "source": [
        "def prepare_train_data(df, features_column, start_date, train_windows):\n",
        "  start_date = pd.to_datetime(start_date)\n",
        "  end_date = start_date + datetime.timedelta(weeks = train_windows)\n",
        "  \n",
        "  df[\"weekStarts\"] = pd.to_datetime(df[\"weekStarts\"])\n",
        "  df[\"weekEnds\"] = pd.to_datetime(df[\"weekEnds\"])\n",
        "  mask = (df[\"weekStarts\"] >= start_date) & (df[\"weekEnds\"] < end_date)\n",
        "  train = df.loc[mask]\n",
        "  X_train=train[features_column]\n",
        "  y_train=train[\"weeklyReturn\"]\n",
        "  return (X_train, y_train)\n",
        "\n",
        "def prepare_test_data(df, features_column, train_start_date, train_windows, test_windows):\n",
        "  start_date = pd.to_datetime(train_start_date)+datetime.timedelta(weeks = train_windows)\n",
        "  end_date = start_date+datetime.timedelta(weeks = test_windows)\n",
        "\n",
        "  df[\"weekStarts\"] = pd.to_datetime(df[\"weekStarts\"])\n",
        "  df[\"weekEnds\"] = pd.to_datetime(df[\"weekEnds\"])\n",
        "  mask = (df[\"weekStarts\"] >= start_date) & (df[\"weekEnds\"] < end_date)\n",
        "  test = df.loc[mask]\n",
        "  X_test=test[features_column]\n",
        "  y_test=test[\"weeklyReturn\"]\n",
        "  return (X_test, y_test)\n",
        "\n",
        "def prepare_trade_data(df, features_column, trade_date):\n",
        "  #not the actual start date of the week been predicted, but the start date of the week before the week been predicted, since GitHub data is lagged for one week\n",
        "  \n",
        "  trade = df.loc[df[\"weekStarts\"].isin([trade_date])]\n",
        "  #print(trade.shape)\n",
        "  X_trade = trade[features_column]\n",
        "  y_trade = trade['weeklyReturn']\n",
        "  trade_tic = trade['ticker']\n",
        "  return (X_trade, y_trade, trade_tic)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGua4UHYVdHD"
      },
      "source": [
        "def evaluate_model(model, X_test, y_test):\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "    #from sklearn.metrics import mean_squared_log_error\n",
        "\n",
        "    from sklearn.metrics import mean_absolute_error\n",
        "    from sklearn.metrics import explained_variance_score\n",
        "    from sklearn.metrics import r2_score\n",
        "    y_predict = model.predict(X_test)\n",
        "\n",
        "    mae = mean_absolute_error(y_test, y_predict)\n",
        "    \n",
        "\n",
        "    mse = mean_squared_error(y_test, y_predict)\n",
        "    #msle = mean_squared_log_error(y_test, y_predict)\n",
        "\n",
        "    explained_variance = explained_variance_score(y_test, y_predict)\n",
        "    r2 = r2_score(y_test, y_predict)\n",
        "\n",
        "    return mae"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tr3Q1PgRroF4",
        "outputId": "dd9f7579-e5dc-4aae-f101-53f223863aae"
      },
      "source": [
        "features_column = ['nstar', 'nfork', 'nissue', 'ncommit', 'nissueClosed', 'npullRequest', 'npullRequestClosed', 'npullRequestMerged']\n",
        "#train_start_date = \"2012-01-07\" #first Saturday in 2012\n",
        "train_windows = 48\n",
        "test_windows = 12\n",
        "\n",
        "def date_range(start_date, end_date):\n",
        "  dates = []\n",
        "  for n in range(0, int((end_date - start_date).days) + 1, 7):\n",
        "    dates.append(start_date + datetime.timedelta(n))\n",
        "  return dates\n",
        "\n",
        "start_date = datetime.date(2012, 1, 7) #Saturday\n",
        "end_date = datetime.date(2020, 2, 22) #Saturday\n",
        "train_start_dates = date_range(start_date, end_date)\n",
        "trade_dates = [x+datetime.timedelta(weeks=train_windows+test_windows) for x in train_start_dates]\n",
        "\n",
        "print(len(train_start_dates))\n",
        "print(trade_dates[-1])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "425\n",
            "2021-04-17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3j3gXasyPg9"
      },
      "source": [
        "evaluation_record = []\n",
        "\n",
        "for i in range(len(trade_dates)):\n",
        "  train_start_date = train_start_dates[i]\n",
        "  trade_date = trade_dates[i]\n",
        "\n",
        "  X_train, y_train = prepare_train_data(df, features_column, train_start_date, train_windows)\n",
        "  X_train_lstm = np.reshape(X_train.values, (X_train.values.shape[0], X_train.values.shape[1], 1))\n",
        "\n",
        "  X_test, y_test = prepare_test_data(df, features_column, train_start_date, train_windows, test_windows)\n",
        "  X_test_lstm = np.reshape(X_test.values, (X_test.values.shape[0], X_test.values.shape[1], 1))\n",
        "\n",
        "  print(\"Trading for week\", trade_date+datetime.timedelta(weeks=1), \"to\", trade_date+datetime.timedelta(13))\n",
        "  X_trade, y_trade, trade_tic = prepare_trade_data(df, features_column, trade_date)\n",
        "  X_trade_lstm = np.reshape(X_trade.values, (X_trade.values.shape[0], X_trade.values.shape[1], 1))\n",
        "\n",
        "   # Train\n",
        "  lr_model = train_linear_regression(X_train, y_train)\n",
        "  lasso_model = train_lasso(X_train, y_train)\n",
        "  ridge_model = train_ridge(X_train, y_train)\n",
        "\n",
        "  rf_model = train_random_forest(X_train, y_train)\n",
        "  svm_model = train_svm(X_train,y_train)\n",
        "  \n",
        "  #gbm_model = train_gbm(X_train, y_train)\n",
        "  #ada_model = train_ada(X_train, y_train)\n",
        "  lstm_model = train_lstm(X_train_lstm, y_train)\n",
        "\n",
        "    # Validation \n",
        "  lr_eval = evaluate_model(lr_model, X_test, y_test)\n",
        "  lasso_eval = evaluate_model(lasso_model, X_test, y_test)\n",
        "  ridge_eval = evaluate_model(ridge_model, X_test, y_test)\n",
        "  \n",
        "  rf_eval = evaluate_model(rf_model, X_test, y_test)\n",
        "  \n",
        "  svm_eval = evaluate_model(svm_model, X_test, y_test)\n",
        "\n",
        "  #gbm_eval = evaluate_model(gbm_model, X_test, y_test)\n",
        "  #ada_eval = evaluate_model(ada_model, X_test, y_test)\n",
        "  \n",
        "  lstm_eval = evaluate_model(lstm_model, X_test_lstm, y_test)\n",
        "\n",
        "        \n",
        "  # Trade\n",
        "  y_trade_lr = lr_model.predict(X_trade)\n",
        "  y_trade_lasso = lasso_model.predict(X_trade)\n",
        "  y_trade_ridge = ridge_model.predict(X_trade)\n",
        "  \n",
        "  y_trade_rf = rf_model.predict(X_trade)\n",
        "  \n",
        "  y_trade_svm = svm_model.predict(X_trade)\n",
        "\n",
        "  #y_trade_gbm = gbm_model.predict(X_trade)\n",
        "  #y_trade_ada = ada_model.predict(X_trade)\n",
        "  y_trade_lstm = lstm_model.predict(X_trade_lstm).flatten()\n",
        "\n",
        "  eval_data = [[lr_eval, y_trade_lr], \n",
        "                    [lasso_eval, y_trade_lasso],\n",
        "                     [ridge_eval, y_trade_ridge],\n",
        "                     [rf_eval, y_trade_rf], \n",
        "                     [svm_eval,y_trade_svm],\n",
        "               #      [gbm_eval,y_trade_gbm],                     \n",
        "               #      [ada_eval,y_trade_ada],\n",
        "                    [lstm_eval,y_trade_lstm]\n",
        "\n",
        "                    ]\n",
        "\n",
        "  eval_table = pd.DataFrame(eval_data, columns=['model_eval', 'model_predict_return'],\n",
        "                                  index=['lr', 'lasso','ridge','rf', 'svm','lstm'])  \n",
        "  \n",
        "  evaluation_record.append((trade_date+datetime.timedelta(weeks=1), eval_table))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jo0lvPQvfxmZ"
      },
      "source": [
        "# Prepare Train Data \n",
        "X_train = (samples, timesteps, features) for lstm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yN0wI029FsmC"
      },
      "source": [
        "def prepare_train_data_2(df, start_date=\"2015-01-02\", train_size=48, window_size=20,\n",
        "                       features_column = ['nstar', 'nfork', 'nissue', 'ncommit', 'nissueClosed', 'npullRequest', 'npullRequestClosed', 'npullRequestMerged']):\n",
        "  '''\n",
        "  input\n",
        "  start_date: always a Saturday (the start day of a \"stock\" week)\n",
        "  train_size: number of weeks included in the training set\n",
        "  window_size: number of weeks in each training sample\n",
        "\n",
        "  return\n",
        "  X_train: (samples, timesteps, features)\n",
        "  '''\n",
        "  \n",
        "  start_date = pd.to_datetime(start_date)\n",
        "  end_date = start_date + datetime.timedelta(weeks = train_size)\n",
        "  print(start_date, end_date)\n",
        "  \n",
        "  companies = pd.read_csv(\"/content/drive/MyDrive/StockML /Data/companyInfo/companies_final.csv\")\n",
        "  X_train = []\n",
        "  y_train = []\n",
        "  for i in range(companies.shape[0]):\n",
        "    if (i<start_company):\n",
        "      continue\n",
        "    if(i>end_company):\n",
        "      break\n",
        "\n",
        "    company = companies.at[i, \"githubUser\"]\n",
        "    df = pd.read_csv(\"/content/drive/MyDrive/StockML /Data/trainData/companies/\"+company+\".csv\")\n",
        "    df[\"weekStarts\"] = pd.to_datetime(df[\"weekStarts\"])\n",
        "    df[\"weekEnds\"] = pd.to_datetime(df[\"weekEnds\"])\n",
        "    mask = (df[\"weekStarts\"] >= start_date) & (df[\"weekEnds\"] < end_date)\n",
        "    df_temp = df.loc[mask].reset_index()\n",
        "    print(company, df_temp.shape)\n",
        "    df_x = df_temp[features_column]\n",
        "    df_y = df_temp[\"weeklyReturn\"]\n",
        "\n",
        "    if (df_temp.shape[0]<window_size): \n",
        "      continue\n",
        "    for j in range(df_temp.shape[0]-window_size):\n",
        "      X_train.append(df_x.values[j:j+window_size, :])\n",
        "      y_train.append(df_y.values[j:j+window_size])\n",
        "  \n",
        "  X_train = np.asarray(X_train)\n",
        "  y_train = np.asarray(y_train)\n",
        "  y_train = np.reshape(y_train[:, -1], (y_train.shape[0], 1))\n",
        "  return (X_train, y_train)\n",
        "    \n",
        "    \n",
        "  \n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "  "
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZUR98l3SVoj"
      },
      "source": [
        "def prepare_test_data_2(start_company=0, end_company=82, train_start_date=\"2021-04-30\", train_size=48, test_size=24, window_size=20,\n",
        "                       features_column = ['nstar', 'nfork', 'nissue', 'ncommit', 'nissueClosed', 'npullRequest', 'npullRequestClosed', 'npullRequestMerged']):\n",
        "  '''\n",
        "  input \n",
        "  test_size: number of weeks included in the testing set\n",
        "\n",
        "  return\n",
        "  X_test: (samples, timesteps, features)\n",
        "  y_test: (samples, timesteps) \n",
        "  '''\n",
        "  \n",
        "  start_date = pd.to_datetime(train_start_date)+datetime.timedelta(weeks = train_size)\n",
        "  end_date = start_date+datetime.timedelta(weeks = test_size)\n",
        "  \n",
        "  companies = pd.read_csv(\"/content/drive/MyDrive/StockML /Data/companyInfo/companies_final.csv\")\n",
        "  X_test = []\n",
        "  y_test = []\n",
        "  for i in range(companies.shape[0]):\n",
        "    if (i<start_company):\n",
        "      continue\n",
        "    if(i>end_company):\n",
        "      break\n",
        "\n",
        "    company = companies.at[i, \"githubUser\"]\n",
        "    df = pd.read_csv(\"/content/drive/MyDrive/StockML /Data/trainData/companies/\"+company+\".csv\")\n",
        "    df[\"weekStarts\"] = pd.to_datetime(df[\"weekStarts\"])\n",
        "    df[\"weekEnds\"] = pd.to_datetime(df[\"weekEnds\"])\n",
        "\n",
        "    mask = (df[\"weekStarts\"] >= start_date) & (df[\"weekEnds\"]< end_date)\n",
        "    df_temp = df.loc[mask].reset_index()\n",
        "    df_x = df_temp[features_column]\n",
        "    df_y = df_temp[\"weeklyReturn\"]\n",
        "\n",
        "    if (df_temp.shape[0]<window_size): \n",
        "      continue\n",
        "    for j in range(df_temp.shape[0]-window_size):\n",
        "      X_test.append(df_x.values[j:j+window_size, :])\n",
        "      y_test.append(df_y.values[j:j+window_size])\n",
        "  \n",
        "  X_test = np.asarray(X_test)\n",
        "  y_test = np.asarray(y_test)\n",
        "  y_test = np.reshape(y_test[:, -1], (y_test.shape[0], 1)) #only select the last data\n",
        "  return (X_test, y_test)\n",
        "    "
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "of4n_fJkUgQu",
        "outputId": "390073e1-4aa5-44e2-cc05-d5fc9717e8ca"
      },
      "source": [
        "train_start_date = \"2015-02-28\"\n",
        "train_size = 48 #number of weeks for training\n",
        "test_size = 24 #number of weeks for testing\n",
        "window_size = 20 #number of weeks in a (training/testing) sample\n",
        "features_column = ['nstar', 'nfork', 'nissue', 'ncommit', 'nissueClosed', 'npullRequest', 'npullRequestClosed', 'npullRequestMerged']\n",
        "\n",
        "X_train, y_train = prepare_train_data(start_company=0, end_company=8, train_size=train_size, window_size=window_size, start_date=train_start_date)\n",
        "print(\"train:\", X_train.shape, y_train.shape)\n",
        "X_test, y_test = prepare_test_data(start_company=0, end_company=8, train_start_date=train_start_date, test_size=test_size, window_size=window_size)\n",
        "print(\"test:\", X_test.shape, y_test.shape)\n",
        "\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-2b23829e675a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfeatures_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'nstar'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nfork'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nissue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ncommit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nissueClosed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'npullRequest'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'npullRequestClosed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'npullRequestMerged'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_company\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_company\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_start_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_company\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_company\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_start_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_start_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: prepare_train_data() got an unexpected keyword argument 'start_company'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yovs_DGqWr_j"
      },
      "source": [
        "lstm_model = train_lstm(X_train, y_train, n_features = len(features_column))\n",
        "lstm_eval = evaluate_model(lstm_model, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxLGoDj-zdb-"
      },
      "source": [
        "## Simple Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-wJC5MYFBjN"
      },
      "source": [
        "import requests\n",
        "company = \"amzn\"\n",
        "response = requests.get(\"https://api.github.com/users/\"+company)\n",
        "data = response.json()\n",
        "created_at = data['created_at']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I8KAQAeHzMt"
      },
      "source": [
        "from pandas_datareader import data as pdr\n",
        "from datetime import date\n",
        "\n",
        "#natural date range (creation date of the company's github page --- 2021-04-30)\n",
        "start_date = pd.to_datetime(created_at[:10]).tz_localize(None)\n",
        "end_date = pd.to_datetime(\"2021-04-30\").tz_localize(None)\n",
        "#for amzn, the first week start on 2014-09-02, the last week end on 2021-04-30\n",
        "start_date = pd.to_datetime(\"2016-09-02\").tz_localize(None)\n",
        "\n",
        "#select the stock data within the date range\n",
        "stockPrice = pd.read_csv(\"/content/drive/MyDrive/StockML /Data/financialData/AMZN.csv\")\n",
        "stockPrice['Date'] = pd.to_datetime(stockPrice['Date'])\n",
        "mask = (pd.to_datetime(stockPrice['Date'])>= start_date) & (pd.to_datetime(stockPrice['Date']) <= end_date)\n",
        "stockDataSelected = stockPrice.loc[mask].reset_index()\n",
        "display(stockDataSelected)\n",
        "\n",
        "#select the cumulative data within the date range\n",
        "cumulativeData = pd.read_csv(\"/content/drive/MyDrive/StockML /Data/processedData/normalizedCumulativeData/amzn.csv\")\n",
        "cumulativeData[\"date\"] = pd.to_datetime(cumulativeData[\"date\"])\n",
        "mask = (cumulativeData['date'] >= start_date) & (cumulativeData['date'] <= end_date)\n",
        "cumulativeDataSelected = cumulativeData.loc[mask].reset_index()\n",
        "display(cumulativeDataSelected)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yt1rIo0LRILT"
      },
      "source": [
        "import scipy.stats\n",
        "import numpy as np\n",
        "\n",
        "x = np.array(list(stockDataSelected['Open']))\n",
        "y = []\n",
        "\n",
        "#remove the dates not in x\n",
        "stockDates = list(stockDataSelected['Date'])\n",
        "for i in range(cumulativeDataSelected.shape[0]):\n",
        "  if(cumulativeDataSelected.at[i, 'date'] in stockDates):\n",
        "    y.append(cumulativeDataSelected.at[i, 'issue'])\n",
        "y = np.array(y)\n",
        "\n",
        "print(x.shape, y.shape)\n",
        "\n",
        "print(\"Pearson:\", scipy.stats.pearsonr(x, y))\n",
        "print(\"Spearman:\", scipy.stats.spearmanr(x, y))\n",
        "print(\"Kendall:\", scipy.stats.kendalltau(x, y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7XYibGwMxWJ"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "#sns.lineplot(x = \"Date\", y = \"Close\", data = stockDataSelected)\n",
        "#sns.lineplot(x = \"date\", y = \"star\", data = cumulativeDataSelected)\n",
        "\n",
        "fig,ax =  plt.subplots( 2, 2, figsize = ( 18, 8))\n",
        "\n",
        "sns.lineplot(x = \"Date\", y = \"Close\", color = \"r\", data = stockDataSelected, ax = ax[0][0])\n",
        "ax[0][0].tick_params(labelrotation = 25)\n",
        "\n",
        "sns.lineplot(x = \"date\", y = \"star\", color = \"g\", data = cumulativeDataSelected, ax = ax[0][1])\n",
        "ax[0][1].tick_params(labelrotation = 25)\n",
        "\n",
        "sns.lineplot(x = \"date\", y = \"commit\", color = \"y\", data = cumulativeDataSelected, ax = ax[1][0])\n",
        "ax[1][0].tick_params(labelrotation = 25)\n",
        "\n",
        "sns.lineplot(x = \"date\", y = \"issue\", color = \"b\", data = cumulativeDataSelected, ax = ax[1][1])\n",
        "ax[1][1].tick_params(labelrotation = 25)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}